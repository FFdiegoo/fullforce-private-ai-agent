#!/usr/bin/env node

/**
 * Process Document Script
 * 
 * This script processes a single document from the documents_metadata table,
 * creates embeddings, and stores them in the vector database for AI retrieval.
 * 
 * Usage: node scripts/process-document.js <document-id>
 */

const { createClient } = require('@supabase/supabase-js');
const { OpenAI } = require('openai');
require('dotenv').config({ path: '.env.local' });

// Configuration
const CONFIG = {
  SUPABASE_URL: process.env.NEXT_PUBLIC_SUPABASE_URL,
  SUPABASE_KEY: process.env.SUPABASE_SERVICE_ROLE_KEY,
  OPENAI_API_KEY: process.env.OPENAI_API_KEY,
  STORAGE_BUCKET: 'company-docs',
  EMBEDDING_MODEL: 'text-embedding-ada-002',
  CHUNK_SIZE: 500,
  CHUNK_OVERLAP: 50
};

// Get document ID from command line arguments
const documentId = process.argv[2];

if (!documentId) {
  console.error('‚ùå Usage: node scripts/process-document.js <document-id>');
  process.exit(1);
}

// Validate configuration
if (!CONFIG.SUPABASE_URL || !CONFIG.SUPABASE_KEY) {
  console.error('‚ùå Missing Supabase credentials. Please check your .env.local file.');
  process.exit(1);
}

if (!CONFIG.OPENAI_API_KEY) {
  console.error('‚ùå Missing OpenAI API key. Please check your .env.local file.');
  process.exit(1);
}

// Initialize clients
const supabase = createClient(CONFIG.SUPABASE_URL, CONFIG.SUPABASE_KEY);
const openai = new OpenAI({ apiKey: CONFIG.OPENAI_API_KEY });

// Main function
async function main() {
  console.log(`üöÄ Processing document with ID: ${documentId}`);
  
  try {
    // Step 1: Get document metadata
    console.log('üîç Fetching document metadata...');
    const { data: document, error: metadataError } = await supabase
      .from('documents_metadata')
      .select('*')
      .eq('id', documentId)
      .single();

    if (metadataError) {
      throw new Error(`Failed to fetch document metadata: ${metadataError.message}`);
    }

    if (!document) {
      throw new Error(`Document with ID ${documentId} not found`);
    }

    console.log(`‚úÖ Found document: ${document.filename}`);

    // Step 2: Download document from Supabase Storage
    console.log(`üì• Downloading from ${document.storage_path}...`);
    const { data: fileData, error: downloadError } = await supabase
      .storage
      .from(CONFIG.STORAGE_BUCKET)
      .download(document.storage_path);

    if (downloadError) {
      throw new Error(`Download failed: ${downloadError.message}`);
    }

    // Step 3: Extract text content
    console.log('üìÑ Extracting text content...');
    const text = await extractText(fileData, document.mime_type);
    
    if (!text || text.trim().length === 0) {
      throw new Error('Failed to extract text or document is empty');
    }
    
    console.log(`‚úÖ Extracted ${text.length} characters`);

    // Step 4: Split text into chunks
    console.log('‚úÇÔ∏è Splitting into chunks...');
    const chunks = splitIntoChunks(text, CONFIG.CHUNK_SIZE, CONFIG.CHUNK_OVERLAP);
    console.log(`‚úÖ Created ${chunks.length} chunks`);

    // Step 5: Generate embeddings for each chunk
    console.log('üß† Generating embeddings...');
    const embeddedChunks = await generateEmbeddings(chunks, document);
    console.log(`‚úÖ Generated ${embeddedChunks.length} embeddings`);

    // Step 6: Store chunks in database
    console.log('üíæ Storing chunks in database...');
    await storeChunks(embeddedChunks);
    console.log(`‚úÖ Stored ${embeddedChunks.length} chunks in database`);

    // Step 7: Mark document as processed
    console.log('‚úÖ Marking document as processed...');
    const { error: updateError } = await supabase
      .from('documents_metadata')
      .update({
        processed: true,
        processed_at: new Date().toISOString()
      })
      .eq('id', document.id);

    if (updateError) {
      throw new Error(`Failed to update document status: ${updateError.message}`);
    }

    console.log(`üéâ Successfully processed document: ${document.filename}`);
    
  } catch (error) {
    console.error(`‚ùå Error: ${error.message}`);
    process.exit(1);
  }
}

// Extract text from document
async function extractText(fileData, mimeType) {
  // For this simple implementation, we'll assume all files are text-based
  // In a production environment, you would use different parsers based on mimeType
  
  try {
    // For text files
    if (mimeType === 'text/plain' || mimeType === 'text/markdown') {
      return new TextDecoder().decode(fileData);
    }
    
    // For other file types, we'd need specific parsers
    // This is a simplified implementation
    return new TextDecoder().decode(fileData);
    
  } catch (error) {
    throw new Error(`Text extraction failed: ${error.message}`);
  }
}

// Split text into chunks with overlap
function splitIntoChunks(text, chunkSize, chunkOverlap) {
  const chunks = [];
  const sentences = text.split(/[.!?]+\s+/);
  let currentChunk = '';

  for (const sentence of sentences) {
    if ((currentChunk + sentence).length > chunkSize && currentChunk.length > 0) {
      chunks.push(currentChunk.trim());
      // Keep the overlap from the previous chunk
      currentChunk = currentChunk.slice(-chunkOverlap) + ' ' + sentence;
    } else {
      currentChunk += (currentChunk ? ' ' : '') + sentence;
    }
  }

  if (currentChunk) {
    chunks.push(currentChunk.trim());
  }

  return chunks;
}

// Generate embeddings for chunks
async function generateEmbeddings(chunks, document) {
  const embeddedChunks = [];

  for (let i = 0; i < chunks.length; i++) {
    try {
      const chunk = chunks[i];
      
      // Generate embedding using OpenAI
      const embeddingResponse = await openai.embeddings.create({
        model: CONFIG.EMBEDDING_MODEL,
        input: chunk,
      });

      const [embedding] = embeddingResponse.data;

      embeddedChunks.push({
        content: chunk,
        embedding: embedding.embedding,
        metadata: {
          id: document.id,
          filename: document.filename,
          storage_path: document.storage_path,
          afdeling: document.afdeling,
          categorie: document.categorie,
          onderwerp: document.onderwerp,
          versie: document.versie
        },
        chunk_index: i
      });
      
      // Small delay to avoid rate limiting
      await new Promise(r => setTimeout(r, 200));
      
    } catch (error) {
      console.error(`‚ö†Ô∏è Error generating embedding for chunk ${i}: ${error.message}`);
      // Continue with other chunks
    }
  }

  return embeddedChunks;
}

// Store chunks in database
async function storeChunks(chunks) {
  for (const chunk of chunks) {
    try {
      const { error } = await supabase
        .from('document_chunks')
        .insert({
          content: chunk.content,
          embedding: chunk.embedding,
          metadata: chunk.metadata,
          chunk_index: chunk.chunk_index
        });

      if (error) {
        throw error;
      }
    } catch (error) {
      console.error(`‚ö†Ô∏è Error storing chunk: ${error.message}`);
      // Continue with other chunks
    }
  }
}

// Start the script
main().catch(console.error);